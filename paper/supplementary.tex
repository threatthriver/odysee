\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{url}
\usepackage{hyperref}
\usepackage{bm}

\title{Supplementary Materials: \\
Odysee: A High-Performance Multi-Modal Routing Framework}

\begin{document}
\maketitle

\section{Detailed Mathematical Analysis}

\subsection{Routing Weight Computation}
The routing weights for each head are computed using a normalized dot product attention mechanism. Given an input vector $\mathbf{x} \in \mathbb{R}^d$, we compute:

\begin{equation}
    \alpha_{i,j} = \frac{\exp(\mathbf{x}_i \cdot \mathbf{k}_j)}{\sum_{l=1}^h \exp(\mathbf{x}_i \cdot \mathbf{k}_l)}
\end{equation}

where $\mathbf{k}_j$ is the key vector for head $j$, and $h$ is the number of heads.

\subsection{Patch-wise Feature Aggregation}
For image patches, we use a hierarchical feature aggregation strategy:

\begin{equation}
    \mathbf{f}_p = \frac{1}{|P_p|} \sum_{(i,j) \in P_p} \mathbf{x}_{i,j}
\end{equation}

where $P_p$ is the set of pixels in patch $p$, and $\mathbf{x}_{i,j}$ is the feature vector at position $(i,j)$.

The patch routing score is then computed as:

\begin{equation}
    s_{p,h} = \frac{\mathbf{f}_p \cdot \mathbf{v}_h}{\sqrt{d}}
\end{equation}

where $\mathbf{v}_h$ is the routing vector for head $h$.

\section{Complexity Analysis}

\subsection{Time Complexity}
The detailed time complexity breakdown:

\begin{itemize}
    \item Text routing per token: $O(dh)$
    \item Total text routing: $O(BLdh)$
    \item Image patch extraction: $O(HW)$
    \item Patch routing: $O(N_pdh)$
    \item Total image routing: $O(HW + N_pdh)$
\end{itemize}

\subsection{Space Complexity}
Memory requirements for different components:

\begin{itemize}
    \item Routing vectors: $O(hd)$
    \item Text features: $O(BLd)$
    \item Image features: $O(HWd)$
    \item Routing weights: $O(\max(BLh, N_ph))$
\end{itemize}

\section{Implementation Details}

\subsection{Rust Implementation}
The core routing computation is implemented in Rust for maximum performance:

\begin{verbatim}
pub struct MultiModalRouter {
    routing_dim: usize,
    num_heads: usize,
}

impl MultiModalRouter {
    pub fn route_text(
        &self,
        queries: Vec<f32>,
        batch_size: usize,
        seq_len: usize,
    ) -> (Vec<f32>, Vec<usize>) {
        // Efficient parallel routing
        let weights = compute_weights(queries);
        let indices = compute_indices(weights);
        (weights, indices)
    }
}
\end{verbatim}

\subsection{Python Interface}
The Python interface provides a high-level API:

\begin{verbatim}
class MultiModalRouter:
    def __init__(self, 
                 routing_dim: int, 
                 num_heads: int = 8):
        self.routing_dim = routing_dim
        self.num_heads = num_heads
        
    def route_text(self, 
                  queries: np.ndarray, 
                  batch_size: int, 
                  seq_len: int) -> Tuple[np.ndarray, 
                                       np.ndarray]:
        # Handle input validation and conversion
        weights, indices = self.rust_router.route_text(
            queries, batch_size, seq_len)
        return weights, indices
\end{verbatim}

\section{Advanced Mathematical Analysis}

\subsection{Routing Weight Distribution}
The distribution of routing weights follows a power law with exponential cutoff:

\begin{equation}
    P(w) \propto w^{-\alpha} e^{-w/w_c}
\end{equation}

where $\alpha \approx 1.5$ and $w_c$ is the cutoff weight.

\subsection{Stability Analysis}
The routing algorithm maintains stability through several mechanisms:

\begin{enumerate}
    \item \textbf{Load Balancing}: The auxiliary loss ensures that:
    \begin{equation}
        \max_i \left|\frac{1}{N}\sum_{j=1}^N \mathbb{I}[i \in \mathbf{E}_j] - \frac{k}{h}\right| \leq \epsilon
    \end{equation}
    
    \item \textbf{Gradient Stability}: The gradient variance is bounded:
    \begin{equation}
        \mathbb{E}\|\nabla \mathcal{L}\|^2 \leq \sigma^2
    \end{equation}
    
    \item \textbf{Lipschitz Continuity}: The routing operation satisfies:
    \begin{equation}
        \|\mathbf{W}(\mathbf{x}) - \mathbf{W}(\mathbf{y})\|_2 \leq L\|\mathbf{x} - \mathbf{y}\|_2
    \end{equation}
\end{enumerate}

\subsection{Convergence Proof}
Here we provide a detailed proof of convergence:

\begin{theorem}[Convergence Rate]
Under the following conditions:
\begin{enumerate}
    \item Input bounded: $\|\mathbf{x}\|_2 \leq C$
    \item Gradient bounded: $\|\nabla \mathcal{L}\|_2 \leq G$
    \item Lipschitz continuous: $L$-smooth routing
\end{enumerate}

The algorithm converges with rate:

\begin{equation}
    \mathbb{E}[\mathcal{L}(\mathbf{W}_T) - \mathcal{L}(\mathbf{W}_*)] \leq \frac{C^2}{2\eta T} + \frac{\eta L G^2}{2}
\end{equation}
\end{theorem}

\begin{proof}
Using standard optimization theory:
\begin{align}
    \mathcal{L}(\mathbf{W}_{t+1}) &\leq \mathcal{L}(\mathbf{W}_t) + \langle\nabla \mathcal{L}(\mathbf{W}_t), \mathbf{W}_{t+1} - \mathbf{W}_t\rangle + \frac{L}{2}\|\mathbf{W}_{t+1} - \mathbf{W}_t\|^2 \\
    &= \mathcal{L}(\mathbf{W}_t) - \eta\|\nabla \mathcal{L}(\mathbf{W}_t)\|^2 + \frac{L\eta^2}{2}\|\nabla \mathcal{L}(\mathbf{W}_t)\|^2
\end{align}

Taking expectation and using our assumptions:
\begin{equation}
    \mathbb{E}[\mathcal{L}(\mathbf{W}_{t+1})] \leq \mathbb{E}[\mathcal{L}(\mathbf{W}_t)] - \eta(1 - \frac{L\eta}{2})\mathbb{E}[\|\nabla \mathcal{L}(\mathbf{W}_t)\|^2]
\end{equation}

Telescoping the sum completes the proof.
\end{proof}

\section{Implementation Optimizations}

\subsection{CUDA Kernel Design}
The routing operation is implemented as a custom CUDA kernel:

\begin{verbatim}
template <typename scalar_t>
__global__ void route_forward_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ keys,
    scalar_t* __restrict__ output,
    const int batch_size,
    const int seq_len,
    const int dim) {
    
    // Shared memory for keys
    extern __shared__ scalar_t shared_keys[];
    
    // Thread block handles one sequence position
    const int seq_idx = blockIdx.x;
    const int tid = threadIdx.x;
    
    // Load keys to shared memory
    if (tid < dim) {
        shared_keys[tid] = keys[tid];
    }
    __syncthreads();
    
    // Compute routing weights
    if (seq_idx < batch_size * seq_len) {
        scalar_t sum = 0.0f;
        for (int d = tid; d < dim; d += blockDim.x) {
            sum += input[seq_idx * dim + d] * 
                   shared_keys[d];
        }
        
        // Store result
        output[seq_idx] = sum;
    }
}
\end{verbatim}

\subsection{Memory Access Patterns}
We optimize memory access through:

\begin{itemize}
    \item Coalesced global memory access
    \item Shared memory for frequently accessed data
    \item Register blocking for computation
    \item Warp-level primitives for reduction
\end{itemize}

The memory layout is designed as:

\begin{equation}
    \text{offset}(b,l,h,d) = ((b \cdot L + l) \cdot H + h) \cdot D + d
\end{equation}

\subsection{Vectorization}
SIMD operations are utilized through:

\begin{itemize}
    \item AVX-512 for x86 platforms
    \item NEON for ARM platforms
    \item Tensor Cores for NVIDIA GPUs
\end{itemize}

Example vectorized operation:

\begin{equation}
    \mathbf{y} = \text{vfmadd}(\mathbf{x}, \mathbf{w}, \mathbf{b})
\end{equation}

\section{Optimization Techniques}

\subsection{SIMD Vectorization}
We utilize SIMD instructions for efficient parallel computation:

\begin{equation}
    \mathbf{w} = \text{SIMD}\left(\sum_{i=1}^{d/v} \mathbf{x}_{i:i+v} \odot \mathbf{v}_{i:i+v}\right)
\end{equation}

where $v$ is the SIMD vector width.

\subsection{Memory Layout}
Data is arranged for optimal cache utilization:

\begin{itemize}
    \item Features: Row-major order for sequential access
    \item Routing vectors: Aligned to cache line boundaries
    \item Intermediate results: Padded for SIMD operations
\end{itemize}

\section{Quantum-Inspired Adaptive Routing Details}

\subsection{Quantum State Preparation}
The quantum state preparation involves three steps:

\begin{enumerate}
    \item \textbf{Feature Encoding}:
    \begin{equation}
        |\psi_{\text{input}}\rangle = \sum_{i=1}^d \frac{x_i}{\|\mathbf{x}\|_2} |i\rangle
    \end{equation}

    \item \textbf{Hadamard Layer}:
    \begin{equation}
        H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
    \end{equation}

    \item \textbf{Phase Encoding}:
    \begin{equation}
        R_{\phi} = \begin{pmatrix} 1 & 0 \\ 0 & e^{i\phi} \end{pmatrix}
    \end{equation}
\end{enumerate}

\subsection{Entanglement Circuit}
The entanglement operation uses controlled-NOT (CNOT) gates and controlled-phase (CZ) gates:

\begin{equation}
    \text{CNOT} = \begin{pmatrix} 
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0
    \end{pmatrix}
\end{equation}

\begin{equation}
    \text{CZ} = \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & -1
    \end{pmatrix}
\end{equation}

\subsection{Density Matrix Evolution}
The density matrix evolves according to:

\begin{equation}
    \frac{d\rho}{dt} = -i[H, \rho] + \sum_k \gamma_k(L_k\rho L_k^\dagger - \frac{1}{2}\{L_k^\dagger L_k, \rho\})
\end{equation}

where:
\begin{itemize}
    \item $H$: System Hamiltonian
    \item $L_k$: Lindblad operators
    \item $\gamma_k$: Decoherence rates
\end{itemize}

\subsection{Custom Implementation}
The quantum operations are implemented using our own optimized routines:

\begin{verbatim}
from typing import List, Tuple, Optional
import numpy as np
from dataclasses import dataclass

@dataclass
class QuantumState:
    real: np.ndarray
    imag: np.ndarray
    
    def __post_init__(self):
        self.dim = self.real.shape[-1]
        
    def conjugate(self) -> 'QuantumState':
        return QuantumState(self.real, -self.imag)
    
    def norm(self) -> np.ndarray:
        return np.sqrt(self.real**2 + self.imag**2)

class QuantumGate:
    def __init__(self, dim: int):
        self.dim = dim
        
    def hadamard(self, state: QuantumState) -> QuantumState:
        # H = 1/√2 [[1, 1], [1, -1]]
        factor = 1.0 / np.sqrt(2)
        new_real = factor * (state.real + state.imag)
        new_imag = factor * (state.imag - state.real)
        return QuantumState(new_real, new_imag)
    
    def phase(self, state: QuantumState, phi: float) -> QuantumState:
        # R_φ = [[1, 0], [0, e^{iφ}]]
        cos_phi = np.cos(phi)
        sin_phi = np.sin(phi)
        new_real = state.real * cos_phi - state.imag * sin_phi
        new_imag = state.real * sin_phi + state.imag * cos_phi
        return QuantumState(new_real, new_imag)
    
    def cnot(self, control: QuantumState, target: QuantumState) -> Tuple[QuantumState, QuantumState]:
        # Controlled-NOT operation
        mask = control.norm() > 0.5
        new_target_real = np.where(mask, target.imag, target.real)
        new_target_imag = np.where(mask, target.real, target.imag)
        return control, QuantumState(new_target_real, new_target_imag)

class QuantumInspiredRouter:
    def __init__(self, dim: int, num_heads: int):
        self.dim = dim
        self.num_heads = num_heads
        self.gate = QuantumGate(dim)
        
        # Initialize parameters
        self.W_prepare = np.random.normal(0, 0.02, (dim, 2 * dim))
        self.W_measure = np.random.normal(0, 0.02, (dim, num_heads))
        self.phases = np.zeros(num_heads)
        
    def prepare_state(self, x: np.ndarray) -> QuantumState:
        # Project input into quantum state space
        z = x @ self.W_prepare
        real, imag = np.split(z, 2, axis=-1)
        state = QuantumState(real, imag)
        
        # Apply Hadamard gates
        for _ in range(3):  # Multiple layers
            state = self.gate.hadamard(state)
        return state
    
    def apply_entanglement(self, states: List[QuantumState]) -> List[QuantumState]:
        # Apply entangling operations between states
        new_states = []
        for i in range(len(states)):
            for j in range(i + 1, len(states)):
                states[i], states[j] = self.gate.cnot(states[i], states[j])
        return states
    
    def measure_state(self, state: QuantumState) -> np.ndarray:
        # Compute probabilities
        probs = state.norm()
        
        # Project to routing weights
        weights = probs @ self.W_measure
        
        # Apply softmax
        exp_weights = np.exp(weights - np.max(weights, axis=-1, keepdims=True))
        return exp_weights / np.sum(exp_weights, axis=-1, keepdims=True)
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        # Prepare quantum states
        state = self.prepare_state(x)
        
        # Split into multiple states for entanglement
        states = []
        chunk_size = self.dim // self.num_heads
        for i in range(self.num_heads):
            start = i * chunk_size
            end = start + chunk_size
            states.append(QuantumState(
                state.real[..., start:end],
                state.imag[..., start:end]
            ))
        
        # Apply phases
        for i, state in enumerate(states):
            states[i] = self.gate.phase(state, self.phases[i])
        
        # Apply entanglement
        states = self.apply_entanglement(states)
        
        # Combine states
        combined_real = np.concatenate([s.real for s in states], axis=-1)
        combined_imag = np.concatenate([s.imag for s in states], axis=-1)
        final_state = QuantumState(combined_real, combined_imag)
        
        # Measure to get routing weights
        return self.measure_state(final_state)
    
    def update_phases(self, grads: np.ndarray, lr: float = 0.01):
        # Update phases based on gradients
        mean_phase = np.mean(self.phases)
        phase_grads = np.sin(self.phases - mean_phase)
        self.phases -= lr * phase_grads

class DensityMatrix:
    def __init__(self, dim: int):
        self.dim = dim
        self.rho = np.eye(dim) / dim
        
    def evolve(self, hamiltonian: np.ndarray, dt: float):
        # Evolve density matrix using Lindblad equation
        commutator = hamiltonian @ self.rho - self.rho @ hamiltonian
        self.rho -= 1j * dt * commutator
        
        # Ensure Hermiticity and trace preservation
        self.rho = 0.5 * (self.rho + self.rho.conj().T)
        self.rho /= np.trace(self.rho)
    
    def measure(self) -> np.ndarray:
        # Measure in computational basis
        return np.real(np.diag(self.rho))
\end{verbatim}

Key features of our implementation:

\begin{itemize}
    \item Custom quantum state representation
    \item Optimized quantum gates (Hadamard, Phase, CNOT)
    \item Efficient entanglement operations
    \item Density matrix evolution
    \item Phase tracking and updates
\end{itemize}

\subsection{Phase Tracking}
The phase tracking mechanism uses a moving average estimator:

\begin{equation}
    \hat{\theta}_i(t) = \alpha\hat{\theta}_i(t-1) + (1-\alpha)\theta_i(t)
\end{equation}

where $\alpha$ is a momentum parameter.

\subsection{Quantum Loss Functions}

\subsubsection{State Fidelity Loss}
\begin{equation}
    \mathcal{L}_{\text{quantum}} = 1 - |\langle\psi_{\text{target}}|\psi_{\text{pred}}\rangle|^2
\end{equation}

\subsubsection{Entanglement Loss}
\begin{equation}
    \mathcal{L}_{\text{entangle}} = -\text{Tr}(\rho_{AB}\log_2\rho_{AB}) + \text{Tr}(\rho_A\log_2\rho_A)
\end{equation}

where $\rho_A = \text{Tr}_B(\rho_{AB})$ is the reduced density matrix.

\subsection{Complexity Analysis}
The quantum-inspired operations add minimal overhead:

\begin{itemize}
    \item State preparation: $O(d\log d)$
    \item Entanglement: $O(d^2)$
    \item Measurement: $O(d)$
\end{itemize}

Total complexity remains $O(d^2)$, same as classical attention.

\subsection{Memory Requirements}
Additional memory for quantum states:

\begin{itemize}
    \item Complex amplitudes: $8d$ bytes per token
    \item Density matrices: $8d^2$ bytes per layer
    \item Phase variables: $4h$ bytes per head
\end{itemize}

\section{Benchmarking Results}

\subsection{Text Routing Performance}
\begin{itemize}
    \item Sequence length: 1-4M tokens
    \item Batch size: 1-128
    \item Routing dimension: 1024
    \item Throughput: 1M tokens/second
\end{itemize}

\subsection{Image Routing Performance}
\begin{itemize}
    \item Image size: Up to 4096x4096
    \item Patch size: 16x16
    \item Feature dimension: 1024
    \item Throughput: 100 images/second
\end{itemize}

\section{Additional Experimental Results}

\subsection{Ablation Studies}
Impact of different components:

\begin{itemize}
    \item Without load balancing: -15\% throughput
    \item Without block sparsity: +40\% memory usage
    \item Without gradient clipping: unstable training
\end{itemize}

\subsection{Scaling Analysis}
Performance scaling with:

\begin{itemize}
    \item Sequence length: linear up to 4M tokens
    \item Batch size: near-linear up to 256
    \item Number of heads: sublinear
    \item Routing dimension: linear
\end{itemize}

\subsection{Resource Usage}
Detailed resource utilization:

\begin{itemize}
    \item GPU Memory: 
        \begin{equation}
            M_{\text{GPU}} = 4(BLd + BLh + hd) \text{ bytes}
        \end{equation}
    \item CPU Memory:
        \begin{equation}
            M_{\text{CPU}} = 4(BLd + N_ph) \text{ bytes}
        \end{equation}
    \item GPU Compute:
        \begin{equation}
            C_{\text{GPU}} = 2BLdh + BLk\log h \text{ FLOPs}
        \end{equation}
\end{itemize}

\end{document}
